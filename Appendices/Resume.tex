\chapter{Synthèse en français}

\section{Contexte théorique}
Le Modèle Standard de la physique des particules (MS) est la théorie actuelle pour décrire la dynamique des particules élémentaires.
Il contient 12 fermions qui interagissent entre eux à travers l'échange de bosons d'interaction.
Ces bosons sont au nombre de 12 : 8 gluons (couplant aux quarks) responsables de l'interaction forte qui assure la stabilité du noyau atomique, les bosons $W^\pm$ et $Z$ responsables de l'interaction faible qui engendre la radioactivité, et le photon responsable de l'électromagnétisme.
Finalement, le boson de Higgs complète le modèle en permettant à plusieurs particules du modèle d'acquérir une masse.

Le MS se base sur la théorie du Lagrangien pour décrire la dynamique des particules.
En plus de l'invariance de Lorentz, on impose au Lagrangien du MS d'être également invariant sous différentes transformations locales (de gauge) des champs de fermions.
Cette contrainte entraîne l'apparition dans le modèle des bosons d'inte\-raction.
Pour décrire l'interaction forte, la symétrie sous $SU(3)_c$ est imposée et engendrera la présence des 8 gluons dans la théorie.
L'interaction électrofaible est générée par la symétrie $SU(2)_L\times U(1)_Y$.

Dans cette théorie, les particules se doivent d'avoir une masse nulle afin de respecter les symétries.
Au contraire, les résultats expérimentaux tendent à favoriser des bosons d'interaction faible de grande masse.
Le mécanisme de brisure spontanée de symétrie, utilisé avec succès en matière condensée, a alors été introduit en théorie des champs pour permettre de générer une masse aux bosons d'interaction faible dynamiquement.
Il consiste à ajouter dans la théorie un champ supplémentaire (le champ de Higgs) soumis à un potentiel possédant un minimum non trivial.
Ce nouveau champ, de par sa valeur non nulle dans son état fondamental, générera une masse aux particules auxquelles il est couplé.
Ce champ générera également un boson additionnel massif scalaire dans la théorie : le boson de Higgs.
En couplant le boson de Higgs à l'interaction électrofaible, le mécanisme de brisure spontanée de symétrie permet de générer une masse pour les bosons $W^\pm$ et $Z$.
Similairement, coupler le boson de Higgs aux fermions permet de générer une masse pour ces derniers.

Le boson de Higgs est la dernière particule élémentaire du MS a avoir été découverte.
Dans les collisions de protons qui ont lieu au LHC, le boson de Higgs sera principalement produit à travers le mécanisme de fusion de gluons (ggH), qui représente $87\%$ de la production totale.
La fusion de boson électrofaibles en un boson de Higgs (VBF) représente $10\%$ de la production totale.
Finalement la production de boson de Higgs par radiation par un boson électrofaible (VH) ou la production en association avec une paire de quarks top (ttH) sont également activement recherchées malgré leur faible contribution.
Une fois produit, le boson de Higgs se désintègre immédiatement.
A la masse observée, un grand nombre de canaux de désintégration sont disponibles pour étudier les couplages du boson de Higgs avec les fermions ($H\rightarrow b\bar{b}$, $H\rightarrow \tau\tau$) ou les bosons électrofaibles ($H\rightarrow WW, ZZ$).
Le mode de désintégration considéré dans cette thèse est le mode di-photon $H\rightarrow \gamma\gamma$.
Bien que le Higgs ne couple pas directement au photon, une boucle de $W$ (et de $t$) permet d'observer cet état bénéficiant d'une très bonne résolution expérimentale malgré son faible rapport d'embranchement.
Cette boucle est également intéressante car pouvant contenir des effets au delà du MS.

Depuis sa découverte en 2012, de nombreuses propriétés du boson de Higgs ont pu être mesurées.
En particulier sa masse, qui est une paramètre libre du MS, a été mesurée en combinant les analyses de plusieurs modes de désintégrations des expériences ATLAS et CMS.
Cette mesure a donné $m_H = 125.09 \pm 0.21 \text{(stat.)} \pm 0.11 \text{(syst.)}$.
Les couplages du Higgs aux différentes particules ont également été mesurés.
En particulier, les rapports des couplages du boson de Higgs observés sur leur valeur dans le MS  ont été mesurés pour les principaux modes de production.
Les résultats sont présentés à la figure \ref{fig:orgfd66e48}.

En 2014, les expériences du LHC ont fini une période de travaux.
Une nouvelle campagne de prise de données est prévue jusqu'en 2018.
Lors de cette nouvelle campagne, les nouvelles conditions expérimentales vont permettre de générer près de dix fois plus de bosons de Higgs que lors de la première campagne.
Grâce à cette augmentation de données, couplée à une amélioration des prédictions théoriques, les incertitudes expérimentales vont rapidement devenir dominantes dans les mesures des propriétés du boson de Higgs.
En particulier pour les couplages, l'incertitude dominante de la campagne de 2012 provenait de l'incertitude sur la résolution du détecteur, avec une contribution importante de l'incertitude sur le terme constant du calorimètre électromagnétique.
La réduction de cette incertitude est un des sujets majeurs de ce manuscrit.

\section{Contexte expérimental}

L'étude expérimentale du boson de Higgs se déroule exclusivement sur les expériences du grand collisionneur de hadrons (LHC).
Cet accélérateur de $27$ km de circonférence, situé à Genève, accélère et fait se collisionner des paquets de protons à une énergie de $13$ TeV ($8$ TeV lors de la première campagne).
La qualité des faisceaux a également été améliorée et contribue grandement à l'augmentation attendue de la statistique lors de la seconde campagne de prise de données.
4 expériences se répartissent autour de l'anneau afin d'étudier les résidus des collisions : ALICE, ATLAS, CMS et LHCb.

L'expérience ATLAS repose sur un détecteur généraliste (fig. \ref{fig:org760cc96}) qui a pour objectif de détecter un maximum de particules issues de la zone d'interaction.
Il a une structure en couche successives de sous-détecteurs qui couvrent une très grande fraction de l'angle solide autour du point d'interaction.
La première couche, le tracker, baigne dans un champ magnétique de 2T et a pour objectif de détecter le passage de particules chargées afin de reconstruire leur trajectoire et évaluer leur impulsion.
La couche suivante, le calorimètre électromagnétique, stoppe et mesure l'énergie des photons et des électrons.
La calorimètre hadronique stoppe et mesure l'énergie des hadrons, qui produisent une gerbe.
Finalement, les chambres à muons sont situées dans la partie la plus externe du détecteur pour mesurer la trajectoire des muons.
Les particules qui n'interagissent que très peu avec la matière, tels les neutrinos, ne seront pas détectés.
Cependant, on peut obtenir des information à leur sujet en mesurant la somme des énergies visible détectées et en appliquant la conservation de l'énergie lors de la collision.

Le calorimètre électromagnétique d'ATLAS est constitué d'une succession de plaques de plomb et d'Argon liquide.
Lorsqu'une particule traverse le plomb, elle va produire plusieurs particules et ainsi réduire l'énergie moyenne par particule.
La succession de ces interactions va engendrer une gerbe électromagnétique.
Lorsque les particules de la gerbe vont traverser l'Argon, elle vont l'ioniser puis les électrons produits dériveront vers une électrode qui sortira un signal électrique du détecteur.
Ce signal sera proportionnel au nombre de particules présentes dans la gerbe et permettra alors d'évaluer l'énergie initiale de la particule.
Le signal sortant du détecteur est transformé puis utilisé pour déterminer l'énergie déposée dans une cellule du calorimètre.
En étudiant la forme des dépôts d'énergie dans le détecteur, on est capable d'identifier les particules ayant traversé le calorimètre ainsi que leur énergie initiale.

\section{Etalonnage du calorimètre électromagnétique}

La mesure de l'énergie initiale d'une particule identifiée se fait en deux principales étapes.
La première consiste en l'utilisation de la simulation pour définir un modèle permettant de lier les variables mesurées par le détecteur aux propriétés de la particule incidente.
Du fait des imprécisions de la simulation, des différences notables persistent entre les performances de la simulation et des données.
En particulier la distribution en masse du boson $Z$ dans son canal électronique montrée en fig. \ref{fig:orgb935e5f} est très différentes entre les données et la simulation, malgré la grande connaissance du $Z$ acquise dans les années 80 et 90.
La calibration in situ a alors pour rôle de de corriger empiriquement les données pour permettre un meilleur accord des données et de la simulation dans le cas du $Z$.
Ces corrections pourront alors être extrapolées à d'autres processus.

Deux facteurs correctifs sont définis.
Le facteur d'échelle $\alpha$ corrige linéairement l'énergie d'un électron.
Le terme constant additionnel de la résolution $c$ évalue l'amplitude de la convolution à effectuer sur la simulation pour obtenir une résolution similaire aux données.
Les valeurs les plus probables de ces facteurs sont obtenues par tests successifs.
Des valeurs hypothétiques des facteurs sont injectées dans la simulation puis la nouvelle distribution de masse est comparée à celle des données avec un test de $\chi^2$.
Les valeurs minimisant le $\chi^2$ maximisent l'accord données/simulation.

Du fait des in-homogénéités du détecteurs, les facteurs correctifs ont été définis comme fonction de $\eta=-ln(tan\frac{\theta}{2})$.
Puisque chaque $Z$ est reconstruit grâce à deux électrons ayant des corrections a priori différentes, on crée des classes de $Z$ ayant des électrons aux propriétés similaires.
On peut alors calculer les facteurs correctifs effectifs (en supposant les corrections des deux électrons identiques) dans chacune des classes.
En combinant les facteurs de chaque classe, on est alors capable d'obtenir les corrections de chaque électron indépendamment en fonction de sa position.

Au début de la seconde campagne, aucune donnée n'était disponible pour mesurer les facteurs correctifs nécessaires pour les premières analyses.
Une stratégie a alors été mise en place pour évaluer des facteurs correctifs pour la seconde campagne, en utilisant les données et la simulation de la première.
Les données brutes (réelles ou simulées) de la première campagne ont été traitées avec les algorithmes de la seconde campagne, puis utilisées pour mesurer des facteurs correctifs (fig. \ref{orgb377ac3}).
Cette méthode ne prend néanmoins pas en compte les changements du détecteur entre les deux campagnes, en particulier l'ajout d'une couche de détecteurs silicium au plus près du faisceau.
Une incertitude a été définie en comparant la simulation avec et sans cette couche additionnelle.
Cette nouvelle incertitude  domine largement les incertitudes d'étalonnage de la première campagne qui ont également été conservées.
Les incertitudes totales pour les pré-recommandations sont données en fig. \ref{org95bdf2b}.


Les résultats de la seconde campagne de prise de données sont cohérent avec ceux de la première et avec les changements du détecteur.
La deuxième campagne s'étale sur les années 2015 à 2018, avec une prise de donnée indépendante chaque année.
Dans le contexte de cette thèse, seulement les données des années 2015 et 2016 ont été analysées.
La mesure des facteurs d'échelle en énergie a été effectuée indépendamment pour chaque année.
En revanche, le terme constant additionnel a été calculé de manière globale pour la seconde campagne.
Les résultats, ainsi que leurs incertitudes statistiques et systématiques sont montrés en fig. \ref{fig:org0ac8cf8} et \ref{fig:org3657743}.
Les incertitudes dominantes sur ces facteurs sont de trois natures.
La source d'incertitude dominante concerne la différence de performance de l'étalonnage pour différentes qualités d'identification des électrons utilisés dans l'analyse.
La seconde source d'incertitude consiste en la différence de performance entre des électrons ayant peu ou beaucoup radié durant la traversée du tracker.
Lors de la première campagne, l'incertitude dominante sur la résolution consistait en un biais lié à la méthodologie.
Ce biais a été réduit d'un facteur 5 en 2015, ce qui a rendu négligeable cette source d'incertitude.
Finalement, l'incertitude totale de résolution a été réduite d'un facteur 3, ce qui réduit considérablement sa contribution dans la mesure des couplages du boson de Higgs.

En 2015, un léger excès a été observé à une masse avoisinant les $750$~GeV dans le canal di-photon.
Du fait des écarts significatifs entre les énergies des électrons issus de la désintégration du boson $Z$ et des photons liés à cet excès, des vérifications de la validité des facteurs correctifs à plus haute énergie ont été effectuées.
La distribution du pic de masse corrigée du boson $Z$ a été étudiée pour différentes énergies des électrons.
Cette étude n'a révélé aucune différence entre les performances des facteurs correctifs à différentes énergies moyennes.

\section{Mesure des couplages du boson de Higgs}

L'analyse des couplages du boson de Higgs vise à tester le Modèle Standard à travers la mesure de différentes sections efficaces de production du boson de Higgs.
Les paramètres d'intérêts pour l'analyse de la seconde campagne ont été changés.
Le modèle du  Simplified Template Cross Section (STXS) propose de s'affranchir des incertitudes théoriques présentes dans le dénominateur des indicateurs précédents et de mesurer directement des sections efficaces avec un minimum de dépendance théoriques.
L'espace de phase du boson de Higgs est alors découpé en plusieurs régions orthogonales entre elles.
Les grandeurs mesurées seront alors les sections efficaces individuelles de chacune de ces régions.
Par exemple, l'espace des phases peut alors être découpé en fonction du mode de production (similairement aux $\mu$), mais également en plusieurs régions en terme de multiplicité de jet ou de $p_T$ du Higgs.
Différents niveaux de découpages sont possibles en fonction de la statistique disponible.
Les résultats de ce manuscrit  présentent les sections efficaces mesurées pour 9 régions dont les définitions sont données en fig. \ref{tab:HGam_mergedbins}.
%(fig. \ref{fig:HGam_results_STXSSumary})

Afin d'obtenir de la sensibilité sur toutes ces régions, certaines étant peu peuplées, les événements di-photon sont triés en 31 catégories optimisées pour être sensibles à différentes régions, en particulier celles correspondant aux modes de production $ttH$, $VH$ et VBF.
Plusieurs catégories visent également à optimiser l'information obtenue sur les événements ggH.
Une vérification sur simulation a permis de valider ces catégories d'analyse et de s'assurer de leur sensibilité à leur région dédiée (fig. \ref{fig:orgc7e9fde}).

La base des analyses dans le canal di-photon repose sur la maximisation d'une fonction de vraisemblance composée d'un modèle de bruit de fond additionné à un modèle de signal appliquée à la distribution de la masse reconstruite du système di-photon.
Les événements retenus dans cette analyse sont des paires de photons de haute qualité, ayant déposé leur énergie dans les meilleures régions du détecteur.
Chacun des photon doit de plus avoir peu de dépôts énergétiques autour de lui.
La masse du système est alors calculée en supposant que les deux photons sont les produits de désintégration d'une particule.
La quasi totalité des événements Higgs sont attendus dans l'intervalle de masse $[123, 127]$~GeV.
Cependant, la sélection accepte des événements appartenant à l'intervalle $[105, 160]$~GeV.
Cette coupure étendue permet d'utiliser les données afin de contraindre la forme du bruit de fond : certains bruits de fond de grande section efficace et étant rejetés de manière efficace par l'analyse seraient trop coûteux en terme de ressources informatiques pour être totalement (et sans doute imparfaitement) simulés.
Ce bruit de fond empirique est donc ajusté sur les données avec une fonction décroissante à peu de degrés de liberté : selon la catégorie, une simple exponentielle ou une exponentielle d'un polynôme du second degré est utilisée.
Le signal est représenté par une Crystal Ball à deux cotés (DSCB voir eq. \ref{eq:org11b12d7}) dont les paramètres sont évalués sur de la simulation pour chaque catégorie.
La valeur centrale de la distribution, reliée à la masse du boson de Higgs, ainsi que sa largeur, reliée à la résolution du détecteur, sont multipliées par des termes variables liés aux incertitudes sur l'énergie étalonnée des photons.
Finalement, la norme du signal correspond au nombre d'événement mesurés qui est donc une fonction linéaire des sections efficaces dans les régions d'espace de phase.

De nombreux phénomènes induisent une incertitude sur le modèle de signal.
En particulier, les incertitudes d'étalonnage induisent des incertitudes sur la position du pic du signal ainsi que sur sa largeur.
Pour évaluer ces incertitudes, une comparaison est faite entre le pic de masse corrigé par les facteurs nominaux et le même pic pour lequel les facteurs correctifs de l'énergie des photons sont modifiés d'une erreur standard.
En comparant les résultats d'un ajustement du modèle de signal, on peut propager les incertitudes au modèle d'ajustement.
86 sources d'incertitudes ont été définies pour l'étalonnage en énergie des photons : 9 concernant la résolution et 77 pour l'échelle d'énergie.
Afin de simplifier le modèle statistique, certaines incertitudes d'échelles ont été fusionnées afin de les réduire à 40 contributions.
L'incertitude totale sur la résolution du signal est de l'ordre de 8\% et celle sur le centre du pic (ou de manière équivalente la masse du boson de Higgs) de l'ordre de 0.3\%.

Les résultats des mesures des régions d'espace de phase sont présentés en fig. \ref{fig:HGam_results_STXSSumary}.
La mesure des rapports des sections efficaces observées sur celles prédites par le MS sont présentées avec le résultat inclusif de la première campagne à la fig. \ref{fig:HGam_results_muProd}.
Aucune déviation significative n'est observée par rapport au Modèle Standard.
La contribution des incertitudes théoriques a été grandement réduite depuis la première campagne grâce à un nouveau calcul de la section efficace de production ggH.
Parmi les incertitudes expérimentales, la résolution, qui était dominante, a vu sa contribution réduite d'un facteur 2 mais demeure parmi les incertitudes dominantes.
